{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs 101\n",
    "In this notebook, we'll work through how to make a convolutional neural network (CNN) using Keras and TensorFlow. If you haven't yet installed these packages, please refer to the Installation notebook.\n",
    "\n",
    "## Theory\n",
    "CNNs are a class of neural networks where one (or more) of the layers contain convolutions. CNNs do not need to consist entirely of convolution layers (and indeed, typically have one or more dense layers following convolutions), but they typically start with several convlutions layers. CNNs are well-suited for image recognition tasks, because they encode information about patterns in neighboring pixels within an input layer. A single \"convolution layer\" actually generally consists of three discrete operations: (1) a convolution operation, (2) an activation operation, and (3) a max pooling operation. We will now discuss each of these further.\n",
    "\n",
    "### Convolutions\n",
    "A _convolution_ is a mathematical operation where the values of a function are combined together with neighboring values to generate a \"smoothed\" output value. Mathematically, we can write a convolution as:\n",
    "\n",
    "$$\n",
    "(f * g)(t) \\equiv \\int_{-\\infty}^\\infty f(\\tau) g(t-\\tau) d\\tau\n",
    "$$\n",
    "\n",
    "In the continuous case, the function $g(x)$ is typically a Gaussian function (or an approximation on a finite domain) or a \"top hat\" window (which has a normalized constant amplitude over some range). For the case of a two dimensional discrete convolution (which is the case for CNNs), given an two-dimensional input image $\\vec{A} \\in \\mathbb{R}^{X\\times Y}$ and a convolutional weights matrix $\\vec{W} \\in \\mathbb{R}^{M \\times N}$, the value in the output matrix $B(i,j)$ is given by:\n",
    "\n",
    "$$\n",
    "B(i,j) \\equiv (A * W)(i, j) = \\sum_{m=-M/2}^{M/2} \\sum_{n=-N/2}^{N/2} W(m, n) \\cdot A(i-m, j-n)\n",
    "$$\n",
    "\n",
    "The convolution thus has the effect of \"mixing together\" information in neighboring pixels in the input image. Initially, the values of the convolution window $\\vec{W}$ are random, but as training progresses the values are updated to extract specific features of the input and hidden layers. These values can be positive or negative, but their magnitude is typically close to 1. When the network is fully trained, the convolution operation can be thought of as recognizing local patters in the data.\n",
    "\n",
    "When applying the convolution, one can specify the _padding_ and the _stride_ to use. _Padding_ is adding additional data to the edges of the input images (typically zeros) so that the size of the output image can be the same as the input image. If there is no padding, and a dimension of the convolution kernel is greater than 1 (i.e., $M > 1$ or $N > 1$), then the output image must necessarily be smaller than the input. The _stride_ specifies how many pixels to translate the starting point of the convolutional filter for an adjacent pixel in the output image. For instance, a stride of 2 means that the convolutional kernel will only use _every other_ pixel as a starting location for the convolutional kernel, decreasing the size of the output image by a factor of 2 along the dimension of the stride.\n",
    "\n",
    "### Max Pooling\n",
    "Following the activation function being applied to each pixel of the output of the convolution, a _max pooling_ step is typically applied. This reduced the dimensionality of the data, which means fewer parameters to train in the next layer. It also serves to emphasize the most prominent features present in an output image.\n",
    "\n",
    "A max pool, as its name implies, divides the image into equal chunks (or pools), and simply returns the maximum value within that pool. A typical size of the max pool is a 2x2 square, and so the max pool returns the largest of those 4 values as the new \"output pixel\". As with the padding and stride options mentioned above in the convolution step, this has the property of reducing the dimensionality of the output layers.\n",
    "\n",
    "#### Visualize it!\n",
    "Try to sketch out what is happening when a max pooling step is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a CNN in Keras\n",
    "Now that we've covered some of the basics of CNNs, let's build one using Keras. For the sake of example, we'll work through the same application as yesterday (classifying hand-written digits) but with using a CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/plaplant/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "# make our model\n",
    "model = Sequential()\n",
    "\n",
    "# add layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Now, we'll read in our dataset, the same one from yesterday. We also need to adjust the dimensions of the data, because of the CNN model we're using. We need to have 4 dimensions for our input data (number of images, x-dimension, y-dimension, color channel). Because we have just a single grayscale color channel, there is only a single rank along the last dimension. Nevertheless, we need to explicitly add it to our training and testing arrays. Also, we need to convert our labels to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model\n",
    "Let's use the same tools as we did yesterday to visualize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                184330    \n",
      "=================================================================\n",
      "Total params: 203,434\n",
      "Trainable params: 203,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file=\"images/cnn_model_plot.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN model plot](images/cnn_model_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "We'll need to compile the model. Let's use the same parameters as yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Now it's time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/plaplant/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.7656 - acc: 0.9212 - val_loss: 0.0873 - val_acc: 0.9726\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0626 - acc: 0.9807 - val_loss: 0.0724 - val_acc: 0.9766\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 117s 2ms/step - loss: 0.0430 - acc: 0.9868 - val_loss: 0.0863 - val_acc: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2ae34780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're at about 98% accurate! Getting better!\n",
    "\n",
    "## Why did it take forever?\n",
    "This time, the training took significantly longer than for the simple DNN. Naively, you might think that there were about twice as many parameters to train, so it should take longer. But it took much more than twice as long to train! The difference is that convolutions are much more computationally intensive to perform, both for the forward and backward propagation. This is part of the reason why GPUs are exceptionally good for ML applications in general and CNNs in particular: they are optimized for these types of operations (because of their primary application in generating computer graphics), and so training goes much faster.\n",
    "\n",
    "# Peeking Under the Hood\n",
    "In this case, the training took a very long time to run. We get some high level information about how the training is progressing, but it'd be nice to have more information. Fortunately, there's a tool that can help us do this. If we're using TensorFlow as the computational backend, the program _TensorBoard_ can be used to give us information about the network as it's running (and after it's been run).\n",
    "\n",
    "To make use of TensorBoard, we add what's known as a `callback` to Keras's `fit` method on a Model object. Then from the command line, we open up TensorBoard and can visualize what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinstantiate our model, add a callback, and train again\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# make a TensorBoard callback\n",
    "cb = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# compile and train the model with our callback\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3,\n",
    "          callbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard\n",
    "While that is running, open up a terminal, and navigate to the directory where these notebooks are. You should see a directory called `logs`. For example:\n",
    "```bash\n",
    "$ ls\n",
    "Installation.ipynb  Lesson2.ipynb  Lesson4.ipynb  data    logs\n",
    "Lesson1.ipynb       Lesson3.ipynb  Lesson5.ipynb  images\n",
    "\n",
    "$ tensorboard --logdir=./logs\n",
    "```\n",
    "This will open up the TensorBoard program, which will give you access to quantities like the accuracy and loss values as a function of training epoch, as well as a visualization of the TensorFlow graph that's been generated by the model. There are a lot of nice features that TensorBoard has in it, so take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
